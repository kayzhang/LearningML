{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation, Flatten\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_file = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "img2captions = {}\n",
    "for row in open(caption_file):\n",
    "    row = row.strip()\n",
    "    row = row.split('\\t')\n",
    "    img = row[0][:len(row[0])-2]\n",
    "    cap = row[1].lower()\n",
    "    if img not in img2captions:\n",
    "        img2captions[img] = []\n",
    "    img2captions[img].append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2captions['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = 'Flickr8k_Dataset/'\n",
    "train_images_file = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train_imgs = [line.strip() for line in open(train_images_file)]\n",
    "print(len(train_imgs), train_imgs[:3])\n",
    "\n",
    "val_images_file = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "val_imgs = [line.strip() for line in open(val_images_file)]\n",
    "print(len(val_imgs), val_imgs[:3])\n",
    "\n",
    "test_images_file = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test_imgs = [line.strip() for line in open(test_images_file)]\n",
    "print(len(test_imgs), test_imgs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_imgs[0]\n",
    "plt.imshow(Image.open(images_dir + '/' + img))\n",
    "print('\\n'.join(img2captions[img]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for understanding Counter\n",
    "counter = Counter()\n",
    "counter.update([\"aaa\", \"bbb\", \"aaa\"])\n",
    "counter.update([\"aaa\", \"ccc\"])\n",
    "counter.update([\"ccc\"])\n",
    "print(len(counter))\n",
    "print(counter)\n",
    "\n",
    "counts = [x for x in counter.items()]\n",
    "print(counts)\n",
    "counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(counts)\n",
    "json.dump(counts, open('counts.json', \"w\"), indent=2)\n",
    "print(counts)\n",
    "words = [w for w, c in counts if c >= 1]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import json\n",
    "\n",
    "word_counter = Counter()\n",
    "n_sample = 0\n",
    "maxlen = 0\n",
    "for img, captions in img2captions.items():\n",
    "    for caption in captions:\n",
    "        n_sample += 1\n",
    "        caption = caption.lower()\n",
    "        caption = str(caption)\n",
    "        tokens = caption.split()\n",
    "        maxlen = max([maxlen,len(tokens)])\n",
    "        word_counter.update(tokens)\n",
    "print('number of sample = ' + str(n_sample))\n",
    "print('max len = ' + str(maxlen))\n",
    "\n",
    "\n",
    "word_counts = [x for x in word_counter.items()]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "json.dump(word_counts, open('word_counts.json', \"w\"), indent=2)\n",
    "\n",
    "vocab = [w for w, c in word_counts if c >= 1]\n",
    "start_word = '<start>'\n",
    "end_word = '<end>'\n",
    "vocab = [start_word, end_word] + vocab\n",
    "print('vocabulary size = %d (<start> and <end> included)'%len(vocab))\n",
    "\n",
    "word2idx = OrderedDict(zip(vocab,range(len(vocab))))\n",
    "idx2word = OrderedDict(zip(range(len(vocab)), vocab))\n",
    "json.dump(word2idx, open('word2idx.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = 'I am a student .'\n",
    "caption = caption.lower()\n",
    "tokens = caption.split()\n",
    "print(caption)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract features for images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will feed these images to VGG-16 to get the encoded images. Hence we need to preprocess the images as the authors of VGG-16 did. The last layer of VGG-16 is the softmax classifier(FC layer with 1000 hidden neurons) which returns the probability of a class. This layer should be removed so as to get a feature representation of an image. We will use the last Dense layer(4096 hidden neurons) after popping the classifier layer. Hence the shape of the encoded image will be (1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((1 + np.squeeze(preprocess(images_dir + '/' + train_imgs[0])))/2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3(weights='inception_v3_weights_tf_dim_ordering_tf_kernels.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "new_input = model.input\n",
    "hidden_layer = model.layers[-2].output\n",
    "\n",
    "model_new = Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryi = model_new.predict(preprocess(images_dir + '/' + train_imgs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess(image)\n",
    "    temp_enc = model_new.predict(image)\n",
    "    temp_enc = np.reshape(temp_enc, temp_enc.shape[1])\n",
    "    return temp_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train = {}\n",
    "for img in tqdm(train_imgs):\n",
    "    encoding_train[img] = encode(images_dir + '/' + img)\n",
    "with open(\"encoded_images_train_inceptionV3.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoding_train, encoded_pickle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_test = {}\n",
    "for img in tqdm(test_imgs):\n",
    "    encoding_test[img] = encode(images_dir + '/' + img)\n",
    "with open(\"encoded_images_test_inceptionV3.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoding_test, encoded_pickle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train = pickle.load(open('encoded_images_train_inceptionV3.p', 'rb'))\n",
    "encoding_test = pickle.load(open('encoded_images_test_inceptionV3.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Preprocess the captions\n",
    "Adding '< start >' and '< end >' to all the captions to indicate the starting and ending of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('flickr8k_train_dataset.txt', 'w')\n",
    "f.write(\"image_id\\tcaptions\\n\")\n",
    "for img in train_imgs:\n",
    "    for cap in img2captions[img]:\n",
    "        f.write(img + \"\\t\" + \"<start> \" + cap +\" <end>\" + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('flickr8k_val_dataset.txt', 'w')\n",
    "f.write(\"image_id\\tcaptions\\n\")\n",
    "for img in val_imgs:\n",
    "    for cap in img2captions[img]:\n",
    "        f.write(img + \"\\t\" + \"<start> \" + cap +\" <end>\" + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('flickr8k_test_dataset.txt', 'w')\n",
    "f.write(\"image_id\\tcaptions\\n\")\n",
    "for img in test_imgs:\n",
    "    for cap in img2captions[img]:\n",
    "        f.write(img + \"\\t\" + \"<start> \" + cap +\" <end>\" + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('flickr8k_train_dataset.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [i for i in df['captions']]\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [i for i in df['image_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = c[-1]\n",
    "a, imgs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a.split():\n",
    "    print (i, \"=>\", word2idx[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = 0\n",
    "for cap in df['captions']:\n",
    "    samples_per_epoch += len(cap.split())-1\n",
    "print(samples_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "vocab_size = len(word2idx)\n",
    "def data_generator(batch_size = 128, split='train'):\n",
    "        partial_caps = []\n",
    "        next_words = []\n",
    "        images = []\n",
    "        \n",
    "        csv_file = 'flickr8k_%s_dataset.txt'%split\n",
    "        encoding_img_feat = pickle.load(open('encoded_images_%s_inceptionV3.p'%split, 'rb'))\n",
    "        df = pd.read_csv(csv_file, delimiter='\\t')\n",
    "        df = df.sample(frac=1)\n",
    "        iter = df.iterrows()\n",
    "        c = []\n",
    "        imgs = []\n",
    "        for i in range(df.shape[0]):\n",
    "            x = next(iter)\n",
    "            c.append(x[1][1])\n",
    "            imgs.append(x[1][0])\n",
    "\n",
    "\n",
    "        count = 0\n",
    "        while True:\n",
    "            for j, text in enumerate(c):\n",
    "                current_image = encoding_img_feat[imgs[j]]\n",
    "                for i in range(len(text.split())-1):\n",
    "                    count+=1\n",
    "                    \n",
    "                    partial = [word2idx[txt] for txt in text.split()[:i+1]]\n",
    "                    partial_caps.append(partial)\n",
    "                    \n",
    "                    # Initializing with zeros to create a one-hot encoding matrix\n",
    "                    # This is what we have to predict\n",
    "                    # Hence initializing it with vocab_size length\n",
    "                    n = np.zeros(vocab_size)\n",
    "                    # Setting the next word to 1 in the one-hot encoded matrix\n",
    "                    n[word2idx[text.split()[i+1]]] = 1\n",
    "                    next_words.append(n)\n",
    "                    \n",
    "                    images.append(current_image)\n",
    "\n",
    "                    if count>=batch_size:\n",
    "                        next_words = np.asarray(next_words)\n",
    "                        images = np.asarray(images)\n",
    "                        partial_caps = sequence.pad_sequences(partial_caps, maxlen=max_len, padding='post')\n",
    "                        yield [[images, partial_caps], next_words]\n",
    "                        partial_caps = []\n",
    "                        next_words = []\n",
    "                        images = []\n",
    "                        count = 0\n",
    "\n",
    "                        \n",
    "train_set = data_generator(split='train')\n",
    "val_set = data_generator(split='val')\n",
    "test_set = data_generator(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image, little girl running in field\n",
    "\n",
    "X1,\t\tX2 (text sequence), \t\t\t\t\t\ty (word)\n",
    "image\t< start >, \t\t\t\t\t\t\t\t\tlittle\n",
    "\n",
    "image\t< start >, little,\t\t\t\t\t\t\tgirl\n",
    "\n",
    "image\t< start >, little, girl, \t\t\t\t\trunning\n",
    "\n",
    "image\t< start >, little, girl, running, \t\t\tin\n",
    "\n",
    "image\t< start >, little, girl, running, in, \t\tfield\n",
    "\n",
    "image\t< start >, little, girl, running, in, field,  < end >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(train_set)\n",
    "print(x[0][0].shape)\n",
    "print(x[0][1][:3])\n",
    "print(np.argmax(x[1][:10], axis=1))\n",
    "print(x[0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "image_model = Sequential([\n",
    "        Dense(embedding_size, input_shape=(2048,), activation='relu'),\n",
    "        RepeatVector(1)\n",
    "    ])\n",
    "word_embedding_model = Sequential([\n",
    "        Embedding(vocab_size, embedding_size, input_length=max_len),\n",
    "        TimeDistributed(Dense(embedding_size, activation='relu'))\n",
    "    ])\n",
    "final_model = Sequential([\n",
    "        Merge([image_model, word_embedding_model], mode='concat', concat_axis=1),\n",
    "        LSTM(256, return_sequences=False),\n",
    "        Dense(vocab_size),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "final_model.summary()\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "steps_per_epoch = samples_per_epoch // batch_size\n",
    "final_model.fit_generator(train_set, \n",
    "                          steps_per_epoch=steps_per_epoch, \n",
    "                          epochs=30,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save_weights('saved_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test the captioning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.load_weights('saved_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_captions(image):\n",
    "    start_word = [\"<start>\"]\n",
    "    e = encode(image)\n",
    "    while True:\n",
    "        print(start_word)\n",
    "        par_caps = [word2idx[i] for i in start_word]\n",
    "        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n",
    "        \n",
    "        preds = final_model.predict([np.array([e]), np.array(par_caps)])\n",
    "        word_pred = idx2word[np.argmax(preds[0])]\n",
    "        start_word.append(word_pred)\n",
    "        \n",
    "        \n",
    "        if word_pred == \"<end>\" or len(start_word) > max_len:\n",
    "            break\n",
    "    print(start_word)\n",
    "    return ' '.join(start_word[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_image = images_dir + '/' + test_imgs[0]\n",
    "plt.imshow(Image.open(try_image))\n",
    "plt.show()\n",
    "print (predict_captions(try_image)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
