{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "#  Author:        TFT\n",
    "#  Written:       4/23/2018\n",
    "#  Last updated:  4/25/2018\n",
    "#\n",
    "#  Execution:     python3 SoftmaxRegression_mnist.py\n",
    "#\n",
    "#  TFT Machine Learning\n",
    "#  Assignment  Week 2-2\n",
    "#\n",
    "#  Complete your code between \"YOUR CODE HERE\" and \"END CODE\".\n",
    "#  Donn't modify other code!\n",
    "#  % python3 SoftmaxRegression_mnist.py\n",
    "#  the accuracy will reach 90% after about 1000 steps of training\n",
    "#----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot = True) # labels are \"one-hot vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_image = mnist.test.images[1]\n",
    "first_image = np.array(first_image, dtype = 'float')\n",
    "pixels = first_image.reshape(28, 28)\n",
    "plt.imshow(pixels, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "iter step 0, training batch loss 2.128017, training batch accuracy 0.578125, test accuracy 0.349900\n",
      "iter step 10, training batch loss 0.651475, training batch accuracy 0.796875, test accuracy 0.781300\n",
      "iter step 20, training batch loss 0.461899, training batch accuracy 0.843750, test accuracy 0.841000\n",
      "iter step 30, training batch loss 0.504684, training batch accuracy 0.875000, test accuracy 0.863100\n",
      "iter step 40, training batch loss 0.463971, training batch accuracy 0.828125, test accuracy 0.873000\n",
      "iter step 50, training batch loss 0.228947, training batch accuracy 0.937500, test accuracy 0.876900\n",
      "iter step 60, training batch loss 0.265438, training batch accuracy 0.906250, test accuracy 0.867700\n",
      "iter step 70, training batch loss 0.317358, training batch accuracy 0.937500, test accuracy 0.873200\n",
      "iter step 80, training batch loss 0.447730, training batch accuracy 0.859375, test accuracy 0.881500\n",
      "iter step 90, training batch loss 0.314277, training batch accuracy 0.921875, test accuracy 0.885400\n",
      "iter step 100, training batch loss 0.441791, training batch accuracy 0.843750, test accuracy 0.885500\n",
      "iter step 110, training batch loss 0.372797, training batch accuracy 0.890625, test accuracy 0.895100\n",
      "iter step 120, training batch loss 0.394915, training batch accuracy 0.890625, test accuracy 0.891900\n",
      "iter step 130, training batch loss 0.283564, training batch accuracy 0.921875, test accuracy 0.899000\n",
      "iter step 140, training batch loss 0.276012, training batch accuracy 0.937500, test accuracy 0.889200\n",
      "iter step 150, training batch loss 0.412478, training batch accuracy 0.921875, test accuracy 0.900400\n",
      "iter step 160, training batch loss 0.238685, training batch accuracy 0.906250, test accuracy 0.899300\n",
      "iter step 170, training batch loss 0.234701, training batch accuracy 0.937500, test accuracy 0.897800\n",
      "iter step 180, training batch loss 0.236761, training batch accuracy 0.921875, test accuracy 0.900600\n",
      "iter step 190, training batch loss 0.407667, training batch accuracy 0.875000, test accuracy 0.892100\n",
      "iter step 200, training batch loss 0.388359, training batch accuracy 0.906250, test accuracy 0.902600\n",
      "iter step 210, training batch loss 0.230128, training batch accuracy 0.890625, test accuracy 0.906100\n",
      "iter step 220, training batch loss 0.228845, training batch accuracy 0.906250, test accuracy 0.901100\n",
      "iter step 230, training batch loss 0.404647, training batch accuracy 0.859375, test accuracy 0.904900\n",
      "iter step 240, training batch loss 0.356228, training batch accuracy 0.875000, test accuracy 0.898500\n",
      "iter step 250, training batch loss 0.346209, training batch accuracy 0.890625, test accuracy 0.893000\n",
      "iter step 260, training batch loss 0.294391, training batch accuracy 0.937500, test accuracy 0.900900\n",
      "iter step 270, training batch loss 0.391614, training batch accuracy 0.875000, test accuracy 0.902200\n",
      "iter step 280, training batch loss 0.324671, training batch accuracy 0.875000, test accuracy 0.904000\n",
      "iter step 290, training batch loss 0.250401, training batch accuracy 0.906250, test accuracy 0.898100\n",
      "iter step 300, training batch loss 0.462550, training batch accuracy 0.828125, test accuracy 0.904700\n",
      "iter step 310, training batch loss 0.321850, training batch accuracy 0.875000, test accuracy 0.904300\n",
      "iter step 320, training batch loss 0.233300, training batch accuracy 0.921875, test accuracy 0.910200\n",
      "iter step 330, training batch loss 0.401778, training batch accuracy 0.890625, test accuracy 0.910300\n",
      "iter step 340, training batch loss 0.391446, training batch accuracy 0.937500, test accuracy 0.900500\n",
      "iter step 350, training batch loss 0.380071, training batch accuracy 0.921875, test accuracy 0.903300\n",
      "iter step 360, training batch loss 0.314484, training batch accuracy 0.921875, test accuracy 0.909400\n",
      "iter step 370, training batch loss 0.238098, training batch accuracy 0.906250, test accuracy 0.911200\n",
      "iter step 380, training batch loss 0.388892, training batch accuracy 0.937500, test accuracy 0.910200\n",
      "iter step 390, training batch loss 0.296047, training batch accuracy 0.921875, test accuracy 0.911600\n",
      "iter step 400, training batch loss 0.213405, training batch accuracy 0.937500, test accuracy 0.911000\n",
      "iter step 410, training batch loss 0.405354, training batch accuracy 0.875000, test accuracy 0.913800\n",
      "iter step 420, training batch loss 0.191715, training batch accuracy 0.953125, test accuracy 0.908500\n",
      "iter step 430, training batch loss 0.227184, training batch accuracy 0.937500, test accuracy 0.901900\n",
      "iter step 440, training batch loss 0.413969, training batch accuracy 0.828125, test accuracy 0.896300\n",
      "iter step 450, training batch loss 0.485009, training batch accuracy 0.875000, test accuracy 0.904200\n",
      "iter step 460, training batch loss 0.352827, training batch accuracy 0.906250, test accuracy 0.899800\n",
      "iter step 470, training batch loss 0.350982, training batch accuracy 0.937500, test accuracy 0.905800\n",
      "iter step 480, training batch loss 0.306978, training batch accuracy 0.890625, test accuracy 0.912600\n",
      "iter step 490, training batch loss 0.319228, training batch accuracy 0.937500, test accuracy 0.915600\n",
      "iter step 500, training batch loss 0.256649, training batch accuracy 0.968750, test accuracy 0.912000\n",
      "iter step 510, training batch loss 0.238126, training batch accuracy 0.921875, test accuracy 0.913100\n",
      "iter step 520, training batch loss 0.196298, training batch accuracy 0.921875, test accuracy 0.913800\n",
      "iter step 530, training batch loss 0.224852, training batch accuracy 0.937500, test accuracy 0.911200\n",
      "iter step 540, training batch loss 0.362436, training batch accuracy 0.859375, test accuracy 0.905500\n",
      "iter step 550, training batch loss 0.349224, training batch accuracy 0.937500, test accuracy 0.908500\n",
      "iter step 560, training batch loss 0.189962, training batch accuracy 0.890625, test accuracy 0.910100\n",
      "iter step 570, training batch loss 0.413756, training batch accuracy 0.828125, test accuracy 0.914500\n",
      "iter step 580, training batch loss 0.651406, training batch accuracy 0.875000, test accuracy 0.909600\n",
      "iter step 590, training batch loss 0.415997, training batch accuracy 0.921875, test accuracy 0.910400\n",
      "iter step 600, training batch loss 0.234797, training batch accuracy 0.937500, test accuracy 0.912300\n",
      "iter step 610, training batch loss 0.349524, training batch accuracy 0.906250, test accuracy 0.918100\n",
      "iter step 620, training batch loss 0.341821, training batch accuracy 0.890625, test accuracy 0.910900\n",
      "iter step 630, training batch loss 0.397712, training batch accuracy 0.875000, test accuracy 0.916100\n",
      "iter step 640, training batch loss 0.378810, training batch accuracy 0.890625, test accuracy 0.911600\n",
      "iter step 650, training batch loss 0.203176, training batch accuracy 0.937500, test accuracy 0.910900\n",
      "iter step 660, training batch loss 0.316125, training batch accuracy 0.921875, test accuracy 0.912000\n",
      "iter step 670, training batch loss 0.387408, training batch accuracy 0.890625, test accuracy 0.915000\n",
      "iter step 680, training batch loss 0.493806, training batch accuracy 0.875000, test accuracy 0.903400\n",
      "iter step 690, training batch loss 0.329334, training batch accuracy 0.921875, test accuracy 0.902700\n",
      "iter step 700, training batch loss 0.843295, training batch accuracy 0.734375, test accuracy 0.899000\n",
      "iter step 710, training batch loss 0.382624, training batch accuracy 0.875000, test accuracy 0.896600\n",
      "iter step 720, training batch loss 0.445460, training batch accuracy 0.906250, test accuracy 0.909100\n",
      "iter step 730, training batch loss 0.385517, training batch accuracy 0.906250, test accuracy 0.906100\n",
      "iter step 740, training batch loss 0.394586, training batch accuracy 0.890625, test accuracy 0.911900\n",
      "iter step 750, training batch loss 0.365961, training batch accuracy 0.921875, test accuracy 0.908000\n",
      "iter step 760, training batch loss 0.384199, training batch accuracy 0.890625, test accuracy 0.908900\n",
      "iter step 770, training batch loss 0.490673, training batch accuracy 0.812500, test accuracy 0.910100\n",
      "iter step 780, training batch loss 0.282633, training batch accuracy 0.875000, test accuracy 0.917000\n",
      "iter step 790, training batch loss 0.139532, training batch accuracy 0.984375, test accuracy 0.916700\n",
      "iter step 800, training batch loss 0.383776, training batch accuracy 0.921875, test accuracy 0.908700\n",
      "iter step 810, training batch loss 0.122551, training batch accuracy 0.953125, test accuracy 0.910400\n",
      "iter step 820, training batch loss 0.290505, training batch accuracy 0.921875, test accuracy 0.916600\n",
      "iter step 830, training batch loss 0.238768, training batch accuracy 0.937500, test accuracy 0.914300\n",
      "iter step 840, training batch loss 0.218828, training batch accuracy 0.937500, test accuracy 0.917200\n",
      "iter step 850, training batch loss 0.069385, training batch accuracy 0.984375, test accuracy 0.912200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter step 860, training batch loss 0.453710, training batch accuracy 0.906250, test accuracy 0.901900\n",
      "iter step 870, training batch loss 0.411099, training batch accuracy 0.906250, test accuracy 0.905500\n",
      "iter step 880, training batch loss 0.185632, training batch accuracy 0.921875, test accuracy 0.915900\n",
      "iter step 890, training batch loss 0.299806, training batch accuracy 0.937500, test accuracy 0.910200\n",
      "iter step 900, training batch loss 0.238618, training batch accuracy 0.906250, test accuracy 0.913300\n",
      "iter step 910, training batch loss 0.265408, training batch accuracy 0.906250, test accuracy 0.911800\n",
      "iter step 920, training batch loss 0.572826, training batch accuracy 0.890625, test accuracy 0.907400\n",
      "iter step 930, training batch loss 0.254164, training batch accuracy 0.937500, test accuracy 0.915700\n",
      "iter step 940, training batch loss 0.361391, training batch accuracy 0.921875, test accuracy 0.904900\n",
      "iter step 950, training batch loss 0.365195, training batch accuracy 0.890625, test accuracy 0.913700\n",
      "iter step 960, training batch loss 0.159067, training batch accuracy 0.953125, test accuracy 0.916600\n",
      "iter step 970, training batch loss 0.100479, training batch accuracy 0.984375, test accuracy 0.916300\n",
      "iter step 980, training batch loss 0.335517, training batch accuracy 0.906250, test accuracy 0.918800\n",
      "iter step 990, training batch loss 0.198411, training batch accuracy 0.953125, test accuracy 0.917700\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot = True) # labels are \"one-hot vectors\"\n",
    "\n",
    "# parameters\n",
    "################################################################################\n",
    "### YOUR CODE HERE: 2 lines\n",
    "### Hint: What data type should we use here? What dimension should W and b have?\n",
    "W = tf.Variable(tf.random_normal(shape = [28*28, 10], stddev = 0.01), name = \"W\")\n",
    "b = tf.Variable(tf.zeros([1, 10]))\n",
    "### END CODE\n",
    "################################################################################\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "max_iter = 1000\n",
    "\n",
    "# inputs & target outputs\n",
    "################################################################################\n",
    "### YOUR CODE HERE: 2 lines\n",
    "### Hint: What data type should we use here? What dimension should x and y_ have?\n",
    "x = tf.placeholder(tf.float32, [None, 28*28], name = \"x_placeholder\")\n",
    "y_ = tf.placeholder(tf.int32, [None, 10], name = \"y__placeholder\")\n",
    "### END CODE\n",
    "################################################################################\n",
    "\n",
    "# creat model\n",
    "################################################################################\n",
    "### YOUR CODE HERE: 1 line\n",
    "y = tf.matmul(x, W) + b\n",
    "### END CODE\n",
    "################################################################################\n",
    "\n",
    "# loss function\n",
    "################################################################################\n",
    "### YOUR CODE HERE: 1 line\n",
    "### Hint: look up \"softmax_cross_entropy_with_logits\" in Tensorflow official documation\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = y, labels = y_, name = \"loss\"))\n",
    "### END CODE\n",
    "################################################################################\n",
    "\n",
    "# optimizer\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# accuracy test\n",
    "################################################################################\n",
    "### YOUR CODE HERE: 1 line\n",
    "### Hint: look up \"tf.argmax\" in Tensorflow official documation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "### END CODE\n",
    "################################################################################\n",
    "\n",
    "# training\n",
    "init = tf.global_variables_initializer()\n",
    "### Hint: remember to initialze Variables; remember to feed placeholders\n",
    "with tf.Session() as sess:\n",
    "    ################################################\n",
    "    ### YOUR CODE HERE: 1 line\n",
    "    sess.run(init)\n",
    "    ### END CODE\n",
    "    ################################################\n",
    "    for iter in range(1000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        test_x = mnist.test.images\n",
    "        test_y = mnist.test.labels\n",
    "        #########################################\n",
    "        ### YOUR CODE HERE: 1 line\n",
    "        sess.run(train_step, {x: batch_x, y_: batch_y}) # run optimizer for one step\n",
    "        ### END CODE\n",
    "        #########################################\n",
    "        if iter%10 == 0:\n",
    "            #########################################\n",
    "            ### YOUR CODE HERE: 3 lines\n",
    "            ### Hint:\n",
    "            ## 1. valuse of loss function on current training batch\n",
    "            ## 2. accuracy on current training batch\n",
    "            ## 3. accuracy on test data\n",
    "            train_loss = sess.run(cross_entropy, {x: batch_x, y_: batch_y})\n",
    "            train_accuracy = sess.run(accuracy, {x: batch_x, y_: batch_y})\n",
    "            test_accuracy = sess.run(accuracy, {x: test_x, y_: test_y})\n",
    "            ### END CODE\n",
    "            #########################################\n",
    "            print(\"iter step %d, training batch loss %f, training batch accuracy %f, test accuracy %f\" %\n",
    "                (iter,train_loss,train_accuracy,test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
