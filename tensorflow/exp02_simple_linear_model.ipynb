{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train a simple linear model by yourself !\n",
      "iter step 0 training loss 34.460297\n",
      "iter step 1 training loss 23.784817\n",
      "iter step 2 training loss 17.899284\n",
      "iter step 3 training loss 14.643126\n",
      "iter step 4 training loss 12.830621\n",
      "iter step 5 training loss 11.811037\n",
      "iter step 6 training loss 11.227242\n",
      "iter step 7 training loss 10.883231\n",
      "iter step 8 training loss 10.671419\n",
      "iter step 9 training loss 10.532749\n",
      "iter step 10 training loss 10.434799\n",
      "iter step 11 training loss 10.359757\n",
      "iter step 12 training loss 10.297836\n",
      "iter step 13 training loss 10.243650\n",
      "iter step 14 training loss 10.194230\n",
      "iter step 15 training loss 10.147932\n",
      "iter step 16 training loss 10.103842\n",
      "iter step 17 training loss 10.061443\n",
      "iter step 18 training loss 10.020445\n",
      "iter step 19 training loss 9.980670\n",
      "iter step 20 training loss 9.942013\n",
      "iter step 21 training loss 9.904401\n",
      "iter step 22 training loss 9.867786\n",
      "iter step 23 training loss 9.832130\n",
      "iter step 24 training loss 9.797402\n",
      "iter step 25 training loss 9.763573\n",
      "iter step 26 training loss 9.730619\n",
      "iter step 27 training loss 9.698514\n",
      "iter step 28 training loss 9.667239\n",
      "iter step 29 training loss 9.636768\n",
      "iter step 30 training loss 9.607084\n",
      "iter step 31 training loss 9.578165\n",
      "iter step 32 training loss 9.549992\n",
      "iter step 33 training loss 9.522543\n",
      "iter step 34 training loss 9.495803\n",
      "iter step 35 training loss 9.469750\n",
      "iter step 36 training loss 9.444372\n",
      "iter step 37 training loss 9.419645\n",
      "iter step 38 training loss 9.395556\n",
      "iter step 39 training loss 9.372088\n",
      "iter step 40 training loss 9.349224\n",
      "iter step 41 training loss 9.326950\n",
      "iter step 42 training loss 9.305251\n",
      "iter step 43 training loss 9.284108\n",
      "iter step 44 training loss 9.263513\n",
      "iter step 45 training loss 9.243446\n",
      "iter step 46 training loss 9.223899\n",
      "iter step 47 training loss 9.204852\n",
      "iter step 48 training loss 9.186299\n",
      "iter step 49 training loss 9.168223\n",
      "iter step 50 training loss 9.150614\n",
      "iter step 51 training loss 9.133457\n",
      "iter step 52 training loss 9.116742\n",
      "iter step 53 training loss 9.100459\n",
      "iter step 54 training loss 9.084597\n",
      "iter step 55 training loss 9.069140\n",
      "iter step 56 training loss 9.054085\n",
      "iter step 57 training loss 9.039415\n",
      "iter step 58 training loss 9.025124\n",
      "iter step 59 training loss 9.011202\n",
      "iter step 60 training loss 8.997638\n",
      "iter step 61 training loss 8.984424\n",
      "iter step 62 training loss 8.971550\n",
      "iter step 63 training loss 8.959007\n",
      "iter step 64 training loss 8.946789\n",
      "iter step 65 training loss 8.934886\n",
      "iter step 66 training loss 8.923288\n",
      "iter step 67 training loss 8.911989\n",
      "iter step 68 training loss 8.900982\n",
      "iter step 69 training loss 8.890259\n",
      "iter step 70 training loss 8.879812\n",
      "iter step 71 training loss 8.869635\n",
      "iter step 72 training loss 8.859717\n",
      "iter step 73 training loss 8.850058\n",
      "iter step 74 training loss 8.840647\n",
      "iter step 75 training loss 8.831478\n",
      "iter step 76 training loss 8.822545\n",
      "iter step 77 training loss 8.813843\n",
      "iter step 78 training loss 8.805365\n",
      "iter step 79 training loss 8.797106\n",
      "iter step 80 training loss 8.789059\n",
      "iter step 81 training loss 8.781219\n",
      "iter step 82 training loss 8.773582\n",
      "iter step 83 training loss 8.766141\n",
      "iter step 84 training loss 8.758892\n",
      "iter step 85 training loss 8.751831\n",
      "iter step 86 training loss 8.744950\n",
      "iter step 87 training loss 8.738248\n",
      "iter step 88 training loss 8.731717\n",
      "iter step 89 training loss 8.725356\n",
      "iter step 90 training loss 8.719158\n",
      "iter step 91 training loss 8.713120\n",
      "iter step 92 training loss 8.707237\n",
      "iter step 93 training loss 8.701508\n",
      "iter step 94 training loss 8.695924\n",
      "iter step 95 training loss 8.690484\n",
      "iter step 96 training loss 8.685184\n",
      "iter step 97 training loss 8.680021\n",
      "iter step 98 training loss 8.674993\n",
      "iter step 99 training loss 8.670093\n",
      "iter step 100 training loss 8.665318\n",
      "iter step 101 training loss 8.660667\n",
      "iter step 102 training loss 8.656137\n",
      "iter step 103 training loss 8.651723\n",
      "iter step 104 training loss 8.647423\n",
      "iter step 105 training loss 8.643233\n",
      "iter step 106 training loss 8.639152\n",
      "iter step 107 training loss 8.635176\n",
      "iter step 108 training loss 8.631301\n",
      "iter step 109 training loss 8.627528\n",
      "iter step 110 training loss 8.623850\n",
      "iter step 111 training loss 8.620268\n",
      "iter step 112 training loss 8.616778\n",
      "iter step 113 training loss 8.613379\n",
      "iter step 114 training loss 8.610065\n",
      "iter step 115 training loss 8.606840\n",
      "iter step 116 training loss 8.603696\n",
      "iter step 117 training loss 8.600632\n",
      "iter step 118 training loss 8.597650\n",
      "iter step 119 training loss 8.594742\n",
      "iter step 120 training loss 8.591909\n",
      "iter step 121 training loss 8.589151\n",
      "iter step 122 training loss 8.586462\n",
      "iter step 123 training loss 8.583845\n",
      "iter step 124 training loss 8.581293\n",
      "iter step 125 training loss 8.578807\n",
      "iter step 126 training loss 8.576385\n",
      "iter step 127 training loss 8.574028\n",
      "iter step 128 training loss 8.571730\n",
      "iter step 129 training loss 8.569489\n",
      "iter step 130 training loss 8.567308\n",
      "iter step 131 training loss 8.565183\n",
      "iter step 132 training loss 8.563113\n",
      "iter step 133 training loss 8.561096\n",
      "iter step 134 training loss 8.559131\n",
      "iter step 135 training loss 8.557217\n",
      "iter step 136 training loss 8.555351\n",
      "iter step 137 training loss 8.553534\n",
      "iter step 138 training loss 8.551764\n",
      "iter step 139 training loss 8.550039\n",
      "iter step 140 training loss 8.548360\n",
      "iter step 141 training loss 8.546722\n",
      "iter step 142 training loss 8.545129\n",
      "iter step 143 training loss 8.543574\n",
      "iter step 144 training loss 8.542061\n",
      "iter step 145 training loss 8.540587\n",
      "iter step 146 training loss 8.539150\n",
      "iter step 147 training loss 8.537751\n",
      "iter step 148 training loss 8.536387\n",
      "iter step 149 training loss 8.535059\n",
      "iter step 150 training loss 8.533765\n",
      "iter step 151 training loss 8.532504\n",
      "iter step 152 training loss 8.531277\n",
      "iter step 153 training loss 8.530079\n",
      "iter step 154 training loss 8.528913\n",
      "iter step 155 training loss 8.527779\n",
      "iter step 156 training loss 8.526671\n",
      "iter step 157 training loss 8.525595\n",
      "iter step 158 training loss 8.524544\n",
      "iter step 159 training loss 8.523520\n",
      "iter step 160 training loss 8.522524\n",
      "iter step 161 training loss 8.521553\n",
      "iter step 162 training loss 8.520607\n",
      "iter step 163 training loss 8.519686\n",
      "iter step 164 training loss 8.518787\n",
      "iter step 165 training loss 8.517913\n",
      "iter step 166 training loss 8.517060\n",
      "iter step 167 training loss 8.516230\n",
      "iter step 168 training loss 8.515421\n",
      "iter step 169 training loss 8.514633\n",
      "iter step 170 training loss 8.513865\n",
      "iter step 171 training loss 8.513118\n",
      "iter step 172 training loss 8.512388\n",
      "iter step 173 training loss 8.511679\n",
      "iter step 174 training loss 8.510987\n",
      "iter step 175 training loss 8.510313\n",
      "iter step 176 training loss 8.509657\n",
      "iter step 177 training loss 8.509019\n",
      "iter step 178 training loss 8.508395\n",
      "iter step 179 training loss 8.507788\n",
      "iter step 180 training loss 8.507196\n",
      "iter step 181 training loss 8.506620\n",
      "iter step 182 training loss 8.506059\n",
      "iter step 183 training loss 8.505512\n",
      "iter step 184 training loss 8.504980\n",
      "iter step 185 training loss 8.504462\n",
      "iter step 186 training loss 8.503954\n",
      "iter step 187 training loss 8.503463\n",
      "iter step 188 training loss 8.502982\n",
      "iter step 189 training loss 8.502516\n",
      "iter step 190 training loss 8.502060\n",
      "iter step 191 training loss 8.501616\n",
      "iter step 192 training loss 8.501184\n",
      "iter step 193 training loss 8.500763\n",
      "iter step 194 training loss 8.500353\n",
      "iter step 195 training loss 8.499952\n",
      "iter step 196 training loss 8.499563\n",
      "iter step 197 training loss 8.499184\n",
      "iter step 198 training loss 8.498815\n",
      "iter step 199 training loss 8.498454\n",
      "iter step 200 training loss 8.498103\n",
      "iter step 201 training loss 8.497762\n",
      "iter step 202 training loss 8.497429\n",
      "iter step 203 training loss 8.497104\n",
      "iter step 204 training loss 8.496788\n",
      "iter step 205 training loss 8.496481\n",
      "iter step 206 training loss 8.496181\n",
      "iter step 207 training loss 8.495889\n",
      "iter step 208 training loss 8.495604\n",
      "iter step 209 training loss 8.495326\n",
      "iter step 210 training loss 8.495057\n",
      "iter step 211 training loss 8.494792\n",
      "iter step 212 training loss 8.494536\n",
      "iter step 213 training loss 8.494287\n",
      "iter step 214 training loss 8.494043\n",
      "iter step 215 training loss 8.493807\n",
      "iter step 216 training loss 8.493575\n",
      "iter step 217 training loss 8.493350\n",
      "iter step 218 training loss 8.493131\n",
      "iter step 219 training loss 8.492918\n",
      "iter step 220 training loss 8.492710\n",
      "iter step 221 training loss 8.492506\n",
      "iter step 222 training loss 8.492309\n",
      "iter step 223 training loss 8.492117\n",
      "iter step 224 training loss 8.491929\n",
      "iter step 225 training loss 8.491746\n",
      "iter step 226 training loss 8.491569\n",
      "iter step 227 training loss 8.491394\n",
      "iter step 228 training loss 8.491226\n",
      "iter step 229 training loss 8.491061\n",
      "iter step 230 training loss 8.490901\n",
      "iter step 231 training loss 8.490746\n",
      "iter step 232 training loss 8.490593\n",
      "iter step 233 training loss 8.490445\n",
      "iter step 234 training loss 8.490300\n",
      "iter step 235 training loss 8.490159\n",
      "iter step 236 training loss 8.490022\n",
      "iter step 237 training loss 8.489889\n",
      "iter step 238 training loss 8.489759\n",
      "iter step 239 training loss 8.489632\n",
      "iter step 240 training loss 8.489509\n",
      "iter step 241 training loss 8.489388\n",
      "iter step 242 training loss 8.489272\n",
      "iter step 243 training loss 8.489157\n",
      "iter step 244 training loss 8.489046\n",
      "iter step 245 training loss 8.488937\n",
      "iter step 246 training loss 8.488832\n",
      "iter step 247 training loss 8.488729\n",
      "iter step 248 training loss 8.488629\n",
      "iter step 249 training loss 8.488531\n",
      "iter step 250 training loss 8.488437\n",
      "iter step 251 training loss 8.488344\n",
      "iter step 252 training loss 8.488254\n",
      "iter step 253 training loss 8.488166\n",
      "iter step 254 training loss 8.488080\n",
      "iter step 255 training loss 8.487996\n",
      "iter step 256 training loss 8.487915\n",
      "iter step 257 training loss 8.487836\n",
      "iter step 258 training loss 8.487759\n",
      "iter step 259 training loss 8.487683\n",
      "iter step 260 training loss 8.487610\n",
      "iter step 261 training loss 8.487538\n",
      "iter step 262 training loss 8.487470\n",
      "iter step 263 training loss 8.487402\n",
      "iter step 264 training loss 8.487336\n",
      "iter step 265 training loss 8.487271\n",
      "iter step 266 training loss 8.487208\n",
      "iter step 267 training loss 8.487148\n",
      "iter step 268 training loss 8.487088\n",
      "iter step 269 training loss 8.487030\n",
      "iter step 270 training loss 8.486975\n",
      "iter step 271 training loss 8.486919\n",
      "iter step 272 training loss 8.486866\n",
      "iter step 273 training loss 8.486814\n",
      "iter step 274 training loss 8.486764\n",
      "iter step 275 training loss 8.486712\n",
      "iter step 276 training loss 8.486665\n",
      "iter step 277 training loss 8.486618\n",
      "iter step 278 training loss 8.486572\n",
      "iter step 279 training loss 8.486527\n",
      "iter step 280 training loss 8.486484\n",
      "iter step 281 training loss 8.486443\n",
      "iter step 282 training loss 8.486401\n",
      "iter step 283 training loss 8.486361\n",
      "iter step 284 training loss 8.486320\n",
      "iter step 285 training loss 8.486283\n",
      "iter step 286 training loss 8.486246\n",
      "iter step 287 training loss 8.486210\n",
      "iter step 288 training loss 8.486174\n",
      "iter step 289 training loss 8.486140\n",
      "iter step 290 training loss 8.486107\n",
      "iter step 291 training loss 8.486073\n",
      "iter step 292 training loss 8.486043\n",
      "iter step 293 training loss 8.486011\n",
      "iter step 294 training loss 8.485981\n",
      "iter step 295 training loss 8.485951\n",
      "iter step 296 training loss 8.485924\n",
      "iter step 297 training loss 8.485895\n",
      "iter step 298 training loss 8.485868\n",
      "iter step 299 training loss 8.485842\n",
      "iter step 300 training loss 8.485816\n",
      "iter step 301 training loss 8.485791\n",
      "iter step 302 training loss 8.485765\n",
      "iter step 303 training loss 8.485743\n",
      "iter step 304 training loss 8.485719\n",
      "iter step 305 training loss 8.485697\n",
      "iter step 306 training loss 8.485675\n",
      "iter step 307 training loss 8.485653\n",
      "iter step 308 training loss 8.485632\n",
      "iter step 309 training loss 8.485612\n",
      "iter step 310 training loss 8.485592\n",
      "iter step 311 training loss 8.485573\n",
      "iter step 312 training loss 8.485553\n",
      "iter step 313 training loss 8.485535\n",
      "iter step 314 training loss 8.485518\n",
      "iter step 315 training loss 8.485500\n",
      "iter step 316 training loss 8.485483\n",
      "iter step 317 training loss 8.485467\n",
      "iter step 318 training loss 8.485451\n",
      "iter step 319 training loss 8.485435\n",
      "iter step 320 training loss 8.485419\n",
      "iter step 321 training loss 8.485404\n",
      "iter step 322 training loss 8.485390\n",
      "iter step 323 training loss 8.485375\n",
      "iter step 324 training loss 8.485361\n",
      "iter step 325 training loss 8.485348\n",
      "iter step 326 training loss 8.485336\n",
      "iter step 327 training loss 8.485324\n",
      "iter step 328 training loss 8.485311\n",
      "iter step 329 training loss 8.485299\n",
      "iter step 330 training loss 8.485287\n",
      "iter step 331 training loss 8.485275\n",
      "iter step 332 training loss 8.485264\n",
      "iter step 333 training loss 8.485253\n",
      "iter step 334 training loss 8.485243\n",
      "iter step 335 training loss 8.485231\n",
      "iter step 336 training loss 8.485222\n",
      "iter step 337 training loss 8.485212\n",
      "iter step 338 training loss 8.485203\n",
      "iter step 339 training loss 8.485193\n",
      "iter step 340 training loss 8.485184\n",
      "iter step 341 training loss 8.485175\n",
      "iter step 342 training loss 8.485167\n",
      "iter step 343 training loss 8.485159\n",
      "iter step 344 training loss 8.485151\n",
      "iter step 345 training loss 8.485142\n",
      "iter step 346 training loss 8.485135\n",
      "iter step 347 training loss 8.485127\n",
      "iter step 348 training loss 8.485121\n",
      "iter step 349 training loss 8.485112\n",
      "iter step 350 training loss 8.485106\n",
      "iter step 351 training loss 8.485099\n",
      "iter step 352 training loss 8.485092\n",
      "iter step 353 training loss 8.485085\n",
      "iter step 354 training loss 8.485079\n",
      "iter step 355 training loss 8.485073\n",
      "iter step 356 training loss 8.485067\n",
      "iter step 357 training loss 8.485062\n",
      "iter step 358 training loss 8.485056\n",
      "iter step 359 training loss 8.485050\n",
      "iter step 360 training loss 8.485045\n",
      "iter step 361 training loss 8.485040\n",
      "iter step 362 training loss 8.485034\n",
      "iter step 363 training loss 8.485030\n",
      "iter step 364 training loss 8.485024\n",
      "iter step 365 training loss 8.485020\n",
      "iter step 366 training loss 8.485015\n",
      "iter step 367 training loss 8.485010\n",
      "iter step 368 training loss 8.485006\n",
      "iter step 369 training loss 8.485002\n",
      "iter step 370 training loss 8.484998\n",
      "iter step 371 training loss 8.484994\n",
      "iter step 372 training loss 8.484990\n",
      "iter step 373 training loss 8.484986\n",
      "iter step 374 training loss 8.484982\n",
      "iter step 375 training loss 8.484979\n",
      "iter step 376 training loss 8.484975\n",
      "iter step 377 training loss 8.484971\n",
      "iter step 378 training loss 8.484968\n",
      "iter step 379 training loss 8.484965\n",
      "iter step 380 training loss 8.484962\n",
      "iter step 381 training loss 8.484960\n",
      "iter step 382 training loss 8.484957\n",
      "iter step 383 training loss 8.484953\n",
      "iter step 384 training loss 8.484949\n",
      "iter step 385 training loss 8.484947\n",
      "iter step 386 training loss 8.484945\n",
      "iter step 387 training loss 8.484942\n",
      "iter step 388 training loss 8.484940\n",
      "iter step 389 training loss 8.484937\n",
      "iter step 390 training loss 8.484935\n",
      "iter step 391 training loss 8.484932\n",
      "iter step 392 training loss 8.484930\n",
      "iter step 393 training loss 8.484928\n",
      "iter step 394 training loss 8.484925\n",
      "iter step 395 training loss 8.484922\n",
      "iter step 396 training loss 8.484921\n",
      "iter step 397 training loss 8.484919\n",
      "iter step 398 training loss 8.484917\n",
      "iter step 399 training loss 8.484915\n",
      "iter step 400 training loss 8.484913\n",
      "iter step 401 training loss 8.484911\n",
      "iter step 402 training loss 8.484909\n",
      "iter step 403 training loss 8.484907\n",
      "iter step 404 training loss 8.484906\n",
      "iter step 405 training loss 8.484903\n",
      "iter step 406 training loss 8.484902\n",
      "iter step 407 training loss 8.484900\n",
      "iter step 408 training loss 8.484900\n",
      "iter step 409 training loss 8.484898\n",
      "iter step 410 training loss 8.484898\n",
      "iter step 411 training loss 8.484895\n",
      "iter step 412 training loss 8.484894\n",
      "iter step 413 training loss 8.484892\n",
      "iter step 414 training loss 8.484891\n",
      "iter step 415 training loss 8.484891\n",
      "iter step 416 training loss 8.484888\n",
      "iter step 417 training loss 8.484887\n",
      "iter step 418 training loss 8.484885\n",
      "iter step 419 training loss 8.484885\n",
      "iter step 420 training loss 8.484884\n",
      "iter step 421 training loss 8.484883\n",
      "iter step 422 training loss 8.484881\n",
      "iter step 423 training loss 8.484880\n",
      "iter step 424 training loss 8.484879\n",
      "iter step 425 training loss 8.484879\n",
      "iter step 426 training loss 8.484878\n",
      "iter step 427 training loss 8.484877\n",
      "iter step 428 training loss 8.484876\n",
      "iter step 429 training loss 8.484875\n",
      "iter step 430 training loss 8.484874\n",
      "iter step 431 training loss 8.484874\n",
      "iter step 432 training loss 8.484872\n",
      "iter step 433 training loss 8.484872\n",
      "iter step 434 training loss 8.484871\n",
      "iter step 435 training loss 8.484870\n",
      "iter step 436 training loss 8.484870\n",
      "iter step 437 training loss 8.484868\n",
      "iter step 438 training loss 8.484868\n",
      "iter step 439 training loss 8.484867\n",
      "iter step 440 training loss 8.484866\n",
      "iter step 441 training loss 8.484865\n",
      "iter step 442 training loss 8.484865\n",
      "iter step 443 training loss 8.484864\n",
      "iter step 444 training loss 8.484863\n",
      "iter step 445 training loss 8.484863\n",
      "iter step 446 training loss 8.484862\n",
      "iter step 447 training loss 8.484861\n",
      "iter step 448 training loss 8.484861\n",
      "iter step 449 training loss 8.484860\n",
      "iter step 450 training loss 8.484861\n",
      "iter step 451 training loss 8.484860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter step 452 training loss 8.484859\n",
      "iter step 453 training loss 8.484859\n",
      "iter step 454 training loss 8.484859\n",
      "iter step 455 training loss 8.484859\n",
      "iter step 456 training loss 8.484859\n",
      "iter step 457 training loss 8.484858\n",
      "iter step 458 training loss 8.484857\n",
      "iter step 459 training loss 8.484856\n",
      "iter step 460 training loss 8.484856\n",
      "iter step 461 training loss 8.484856\n",
      "iter step 462 training loss 8.484855\n",
      "iter step 463 training loss 8.484855\n",
      "iter step 464 training loss 8.484855\n",
      "iter step 465 training loss 8.484855\n",
      "iter step 466 training loss 8.484854\n",
      "iter step 467 training loss 8.484854\n",
      "iter step 468 training loss 8.484853\n",
      "iter step 469 training loss 8.484853\n",
      "iter step 470 training loss 8.484853\n",
      "iter step 471 training loss 8.484853\n",
      "iter step 472 training loss 8.484853\n",
      "iter step 473 training loss 8.484852\n",
      "iter step 474 training loss 8.484852\n",
      "iter step 475 training loss 8.484852\n",
      "iter step 476 training loss 8.484851\n",
      "iter step 477 training loss 8.484850\n",
      "iter step 478 training loss 8.484850\n",
      "iter step 479 training loss 8.484851\n",
      "iter step 480 training loss 8.484850\n",
      "iter step 481 training loss 8.484850\n",
      "iter step 482 training loss 8.484849\n",
      "iter step 483 training loss 8.484849\n",
      "iter step 484 training loss 8.484849\n",
      "iter step 485 training loss 8.484849\n",
      "iter step 486 training loss 8.484848\n",
      "iter step 487 training loss 8.484848\n",
      "iter step 488 training loss 8.484849\n",
      "iter step 489 training loss 8.484849\n",
      "iter step 490 training loss 8.484848\n",
      "iter step 491 training loss 8.484848\n",
      "iter step 492 training loss 8.484848\n",
      "iter step 493 training loss 8.484848\n",
      "iter step 494 training loss 8.484847\n",
      "iter step 495 training loss 8.484847\n",
      "iter step 496 training loss 8.484847\n",
      "iter step 497 training loss 8.484847\n",
      "iter step 498 training loss 8.484846\n",
      "iter step 499 training loss 8.484847\n",
      "iter step 500 training loss 8.484846\n",
      "iter step 501 training loss 8.484847\n",
      "iter step 502 training loss 8.484845\n",
      "iter step 503 training loss 8.484846\n",
      "iter step 504 training loss 8.484846\n",
      "iter step 505 training loss 8.484845\n",
      "iter step 506 training loss 8.484846\n",
      "iter step 507 training loss 8.484845\n",
      "iter step 508 training loss 8.484845\n",
      "iter step 509 training loss 8.484845\n",
      "iter step 510 training loss 8.484845\n",
      "iter step 511 training loss 8.484845\n",
      "iter step 512 training loss 8.484845\n",
      "iter step 513 training loss 8.484844\n",
      "iter step 514 training loss 8.484844\n",
      "iter step 515 training loss 8.484845\n",
      "iter step 516 training loss 8.484845\n",
      "iter step 517 training loss 8.484844\n",
      "iter step 518 training loss 8.484844\n",
      "iter step 519 training loss 8.484844\n",
      "iter step 520 training loss 8.484845\n",
      "iter step 521 training loss 8.484844\n",
      "iter step 522 training loss 8.484844\n",
      "iter step 523 training loss 8.484843\n",
      "iter step 524 training loss 8.484844\n",
      "iter step 525 training loss 8.484844\n",
      "iter step 526 training loss 8.484843\n",
      "iter step 527 training loss 8.484844\n",
      "iter step 528 training loss 8.484843\n",
      "iter step 529 training loss 8.484844\n",
      "iter step 530 training loss 8.484843\n",
      "iter step 531 training loss 8.484843\n",
      "iter step 532 training loss 8.484844\n",
      "iter step 533 training loss 8.484843\n",
      "iter step 534 training loss 8.484842\n",
      "iter step 535 training loss 8.484843\n",
      "iter step 536 training loss 8.484844\n",
      "iter step 537 training loss 8.484844\n",
      "iter step 538 training loss 8.484843\n",
      "iter step 539 training loss 8.484843\n",
      "iter step 540 training loss 8.484843\n",
      "iter step 541 training loss 8.484843\n",
      "iter step 542 training loss 8.484843\n",
      "iter step 543 training loss 8.484843\n",
      "iter step 544 training loss 8.484843\n",
      "iter step 545 training loss 8.484843\n",
      "iter step 546 training loss 8.484843\n",
      "iter step 547 training loss 8.484842\n",
      "iter step 548 training loss 8.484842\n",
      "iter step 549 training loss 8.484842\n",
      "iter step 550 training loss 8.484842\n",
      "iter step 551 training loss 8.484842\n",
      "iter step 552 training loss 8.484842\n",
      "iter step 553 training loss 8.484841\n",
      "iter step 554 training loss 8.484843\n",
      "iter step 555 training loss 8.484843\n",
      "iter step 556 training loss 8.484842\n",
      "iter step 557 training loss 8.484842\n",
      "iter step 558 training loss 8.484842\n",
      "iter step 559 training loss 8.484842\n",
      "iter step 560 training loss 8.484842\n",
      "iter step 561 training loss 8.484842\n",
      "iter step 562 training loss 8.484842\n",
      "iter step 563 training loss 8.484842\n",
      "iter step 564 training loss 8.484842\n",
      "iter step 565 training loss 8.484843\n",
      "iter step 566 training loss 8.484841\n",
      "iter step 567 training loss 8.484842\n",
      "iter step 568 training loss 8.484841\n",
      "iter step 569 training loss 8.484843\n",
      "iter step 570 training loss 8.484842\n",
      "iter step 571 training loss 8.484841\n",
      "iter step 572 training loss 8.484842\n",
      "iter step 573 training loss 8.484841\n",
      "iter step 574 training loss 8.484841\n",
      "iter step 575 training loss 8.484842\n",
      "iter step 576 training loss 8.484842\n",
      "iter step 577 training loss 8.484842\n",
      "iter step 578 training loss 8.484842\n",
      "iter step 579 training loss 8.484842\n",
      "iter step 580 training loss 8.484842\n",
      "iter step 581 training loss 8.484841\n",
      "iter step 582 training loss 8.484842\n",
      "iter step 583 training loss 8.484842\n",
      "iter step 584 training loss 8.484842\n",
      "iter step 585 training loss 8.484842\n",
      "iter step 586 training loss 8.484841\n",
      "iter step 587 training loss 8.484841\n",
      "iter step 588 training loss 8.484841\n",
      "iter step 589 training loss 8.484842\n",
      "iter step 590 training loss 8.484841\n",
      "iter step 591 training loss 8.484842\n",
      "iter step 592 training loss 8.484842\n",
      "iter step 593 training loss 8.484841\n",
      "iter step 594 training loss 8.484841\n",
      "iter step 595 training loss 8.484841\n",
      "iter step 596 training loss 8.484842\n",
      "iter step 597 training loss 8.484841\n",
      "iter step 598 training loss 8.484841\n",
      "iter step 599 training loss 8.484842\n",
      "iter step 600 training loss 8.484842\n",
      "iter step 601 training loss 8.484842\n",
      "iter step 602 training loss 8.484841\n",
      "iter step 603 training loss 8.484841\n",
      "iter step 604 training loss 8.484841\n",
      "iter step 605 training loss 8.484841\n",
      "iter step 606 training loss 8.484841\n",
      "iter step 607 training loss 8.484842\n",
      "iter step 608 training loss 8.484841\n",
      "iter step 609 training loss 8.484841\n",
      "iter step 610 training loss 8.484842\n",
      "iter step 611 training loss 8.484841\n",
      "iter step 612 training loss 8.484841\n",
      "iter step 613 training loss 8.484841\n",
      "iter step 614 training loss 8.484842\n",
      "iter step 615 training loss 8.484841\n",
      "iter step 616 training loss 8.484842\n",
      "iter step 617 training loss 8.484841\n",
      "iter step 618 training loss 8.484842\n",
      "iter step 619 training loss 8.484842\n",
      "iter step 620 training loss 8.484841\n",
      "iter step 621 training loss 8.484841\n",
      "iter step 622 training loss 8.484841\n",
      "iter step 623 training loss 8.484841\n",
      "iter step 624 training loss 8.484841\n",
      "iter step 625 training loss 8.484841\n",
      "iter step 626 training loss 8.484841\n",
      "iter step 627 training loss 8.484841\n",
      "iter step 628 training loss 8.484842\n",
      "iter step 629 training loss 8.484841\n",
      "iter step 630 training loss 8.484841\n",
      "iter step 631 training loss 8.484841\n",
      "iter step 632 training loss 8.484842\n",
      "iter step 633 training loss 8.484841\n",
      "iter step 634 training loss 8.484841\n",
      "iter step 635 training loss 8.484841\n",
      "iter step 636 training loss 8.484840\n",
      "iter step 637 training loss 8.484841\n",
      "iter step 638 training loss 8.484841\n",
      "iter step 639 training loss 8.484842\n",
      "iter step 640 training loss 8.484841\n",
      "iter step 641 training loss 8.484841\n",
      "iter step 642 training loss 8.484840\n",
      "iter step 643 training loss 8.484841\n",
      "iter step 644 training loss 8.484841\n",
      "iter step 645 training loss 8.484842\n",
      "iter step 646 training loss 8.484842\n",
      "iter step 647 training loss 8.484841\n",
      "iter step 648 training loss 8.484841\n",
      "iter step 649 training loss 8.484841\n",
      "iter step 650 training loss 8.484841\n",
      "iter step 651 training loss 8.484841\n",
      "iter step 652 training loss 8.484841\n",
      "iter step 653 training loss 8.484841\n",
      "iter step 654 training loss 8.484841\n",
      "iter step 655 training loss 8.484842\n",
      "iter step 656 training loss 8.484841\n",
      "iter step 657 training loss 8.484841\n",
      "iter step 658 training loss 8.484840\n",
      "iter step 659 training loss 8.484842\n",
      "iter step 660 training loss 8.484840\n",
      "iter step 661 training loss 8.484841\n",
      "iter step 662 training loss 8.484840\n",
      "iter step 663 training loss 8.484840\n",
      "iter step 664 training loss 8.484841\n",
      "iter step 665 training loss 8.484841\n",
      "iter step 666 training loss 8.484841\n",
      "iter step 667 training loss 8.484840\n",
      "iter step 668 training loss 8.484841\n",
      "iter step 669 training loss 8.484842\n",
      "iter step 670 training loss 8.484841\n",
      "iter step 671 training loss 8.484841\n",
      "iter step 672 training loss 8.484841\n",
      "iter step 673 training loss 8.484840\n",
      "iter step 674 training loss 8.484842\n",
      "iter step 675 training loss 8.484840\n",
      "iter step 676 training loss 8.484841\n",
      "iter step 677 training loss 8.484841\n",
      "iter step 678 training loss 8.484841\n",
      "iter step 679 training loss 8.484841\n",
      "iter step 680 training loss 8.484840\n",
      "iter step 681 training loss 8.484841\n",
      "iter step 682 training loss 8.484841\n",
      "iter step 683 training loss 8.484841\n",
      "iter step 684 training loss 8.484840\n",
      "iter step 685 training loss 8.484841\n",
      "iter step 686 training loss 8.484842\n",
      "iter step 687 training loss 8.484842\n",
      "iter step 688 training loss 8.484841\n",
      "iter step 689 training loss 8.484841\n",
      "iter step 690 training loss 8.484841\n",
      "iter step 691 training loss 8.484841\n",
      "iter step 692 training loss 8.484840\n",
      "iter step 693 training loss 8.484841\n",
      "iter step 694 training loss 8.484842\n",
      "iter step 695 training loss 8.484841\n",
      "iter step 696 training loss 8.484841\n",
      "iter step 697 training loss 8.484841\n",
      "iter step 698 training loss 8.484840\n",
      "iter step 699 training loss 8.484840\n",
      "iter step 700 training loss 8.484841\n",
      "iter step 701 training loss 8.484841\n",
      "iter step 702 training loss 8.484840\n",
      "iter step 703 training loss 8.484841\n",
      "iter step 704 training loss 8.484841\n",
      "iter step 705 training loss 8.484841\n",
      "iter step 706 training loss 8.484840\n",
      "iter step 707 training loss 8.484841\n",
      "iter step 708 training loss 8.484841\n",
      "iter step 709 training loss 8.484841\n",
      "iter step 710 training loss 8.484840\n",
      "iter step 711 training loss 8.484840\n",
      "iter step 712 training loss 8.484841\n",
      "iter step 713 training loss 8.484841\n",
      "iter step 714 training loss 8.484841\n",
      "iter step 715 training loss 8.484840\n",
      "iter step 716 training loss 8.484840\n",
      "iter step 717 training loss 8.484841\n",
      "iter step 718 training loss 8.484841\n",
      "iter step 719 training loss 8.484841\n",
      "iter step 720 training loss 8.484840\n",
      "iter step 721 training loss 8.484841\n",
      "iter step 722 training loss 8.484841\n",
      "iter step 723 training loss 8.484841\n",
      "iter step 724 training loss 8.484841\n",
      "iter step 725 training loss 8.484842\n",
      "iter step 726 training loss 8.484841\n",
      "iter step 727 training loss 8.484842\n",
      "iter step 728 training loss 8.484841\n",
      "iter step 729 training loss 8.484842\n",
      "iter step 730 training loss 8.484842\n",
      "iter step 731 training loss 8.484841\n",
      "iter step 732 training loss 8.484841\n",
      "iter step 733 training loss 8.484841\n",
      "iter step 734 training loss 8.484841\n",
      "iter step 735 training loss 8.484841\n",
      "iter step 736 training loss 8.484841\n",
      "iter step 737 training loss 8.484840\n",
      "iter step 738 training loss 8.484841\n",
      "iter step 739 training loss 8.484841\n",
      "iter step 740 training loss 8.484842\n",
      "iter step 741 training loss 8.484840\n",
      "iter step 742 training loss 8.484840\n",
      "iter step 743 training loss 8.484842\n",
      "iter step 744 training loss 8.484841\n",
      "iter step 745 training loss 8.484841\n",
      "iter step 746 training loss 8.484841\n",
      "iter step 747 training loss 8.484841\n",
      "iter step 748 training loss 8.484841\n",
      "iter step 749 training loss 8.484841\n",
      "iter step 750 training loss 8.484840\n",
      "iter step 751 training loss 8.484840\n",
      "iter step 752 training loss 8.484841\n",
      "iter step 753 training loss 8.484841\n",
      "iter step 754 training loss 8.484841\n",
      "iter step 755 training loss 8.484841\n",
      "iter step 756 training loss 8.484842\n",
      "iter step 757 training loss 8.484841\n",
      "iter step 758 training loss 8.484842\n",
      "iter step 759 training loss 8.484841\n",
      "iter step 760 training loss 8.484841\n",
      "iter step 761 training loss 8.484841\n",
      "iter step 762 training loss 8.484841\n",
      "iter step 763 training loss 8.484841\n",
      "iter step 764 training loss 8.484841\n",
      "iter step 765 training loss 8.484841\n",
      "iter step 766 training loss 8.484841\n",
      "iter step 767 training loss 8.484840\n",
      "iter step 768 training loss 8.484840\n",
      "iter step 769 training loss 8.484841\n",
      "iter step 770 training loss 8.484840\n",
      "iter step 771 training loss 8.484841\n",
      "iter step 772 training loss 8.484840\n",
      "iter step 773 training loss 8.484841\n",
      "iter step 774 training loss 8.484842\n",
      "iter step 775 training loss 8.484841\n",
      "iter step 776 training loss 8.484840\n",
      "iter step 777 training loss 8.484840\n",
      "iter step 778 training loss 8.484841\n",
      "iter step 779 training loss 8.484840\n",
      "iter step 780 training loss 8.484841\n",
      "iter step 781 training loss 8.484841\n",
      "iter step 782 training loss 8.484841\n",
      "iter step 783 training loss 8.484841\n",
      "iter step 784 training loss 8.484841\n",
      "iter step 785 training loss 8.484841\n",
      "iter step 786 training loss 8.484841\n",
      "iter step 787 training loss 8.484842\n",
      "iter step 788 training loss 8.484841\n",
      "iter step 789 training loss 8.484840\n",
      "iter step 790 training loss 8.484840\n",
      "iter step 791 training loss 8.484840\n",
      "iter step 792 training loss 8.484840\n",
      "iter step 793 training loss 8.484841\n",
      "iter step 794 training loss 8.484841\n",
      "iter step 795 training loss 8.484841\n",
      "iter step 796 training loss 8.484842\n",
      "iter step 797 training loss 8.484841\n",
      "iter step 798 training loss 8.484841\n",
      "iter step 799 training loss 8.484841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter step 800 training loss 8.484841\n",
      "iter step 801 training loss 8.484841\n",
      "iter step 802 training loss 8.484840\n",
      "iter step 803 training loss 8.484841\n",
      "iter step 804 training loss 8.484841\n",
      "iter step 805 training loss 8.484841\n",
      "iter step 806 training loss 8.484841\n",
      "iter step 807 training loss 8.484841\n",
      "iter step 808 training loss 8.484841\n",
      "iter step 809 training loss 8.484841\n",
      "iter step 810 training loss 8.484841\n",
      "iter step 811 training loss 8.484841\n",
      "iter step 812 training loss 8.484841\n",
      "iter step 813 training loss 8.484841\n",
      "iter step 814 training loss 8.484841\n",
      "iter step 815 training loss 8.484842\n",
      "iter step 816 training loss 8.484840\n",
      "iter step 817 training loss 8.484842\n",
      "iter step 818 training loss 8.484840\n",
      "iter step 819 training loss 8.484840\n",
      "iter step 820 training loss 8.484840\n",
      "iter step 821 training loss 8.484840\n",
      "iter step 822 training loss 8.484841\n",
      "iter step 823 training loss 8.484840\n",
      "iter step 824 training loss 8.484841\n",
      "iter step 825 training loss 8.484840\n",
      "iter step 826 training loss 8.484841\n",
      "iter step 827 training loss 8.484841\n",
      "iter step 828 training loss 8.484840\n",
      "iter step 829 training loss 8.484841\n",
      "iter step 830 training loss 8.484842\n",
      "iter step 831 training loss 8.484841\n",
      "iter step 832 training loss 8.484842\n",
      "iter step 833 training loss 8.484842\n",
      "iter step 834 training loss 8.484841\n",
      "iter step 835 training loss 8.484841\n",
      "iter step 836 training loss 8.484842\n",
      "iter step 837 training loss 8.484841\n",
      "iter step 838 training loss 8.484841\n",
      "iter step 839 training loss 8.484841\n",
      "iter step 840 training loss 8.484841\n",
      "iter step 841 training loss 8.484840\n",
      "iter step 842 training loss 8.484840\n",
      "iter step 843 training loss 8.484840\n",
      "iter step 844 training loss 8.484841\n",
      "iter step 845 training loss 8.484840\n",
      "iter step 846 training loss 8.484841\n",
      "iter step 847 training loss 8.484840\n",
      "iter step 848 training loss 8.484840\n",
      "iter step 849 training loss 8.484841\n",
      "iter step 850 training loss 8.484841\n",
      "iter step 851 training loss 8.484841\n",
      "iter step 852 training loss 8.484841\n",
      "iter step 853 training loss 8.484841\n",
      "iter step 854 training loss 8.484841\n",
      "iter step 855 training loss 8.484841\n",
      "iter step 856 training loss 8.484841\n",
      "iter step 857 training loss 8.484841\n",
      "iter step 858 training loss 8.484841\n",
      "iter step 859 training loss 8.484841\n",
      "iter step 860 training loss 8.484841\n",
      "iter step 861 training loss 8.484841\n",
      "iter step 862 training loss 8.484841\n",
      "iter step 863 training loss 8.484842\n",
      "iter step 864 training loss 8.484841\n",
      "iter step 865 training loss 8.484841\n",
      "iter step 866 training loss 8.484840\n",
      "iter step 867 training loss 8.484840\n",
      "iter step 868 training loss 8.484840\n",
      "iter step 869 training loss 8.484840\n",
      "iter step 870 training loss 8.484840\n",
      "iter step 871 training loss 8.484840\n",
      "iter step 872 training loss 8.484841\n",
      "iter step 873 training loss 8.484840\n",
      "iter step 874 training loss 8.484840\n",
      "iter step 875 training loss 8.484840\n",
      "iter step 876 training loss 8.484840\n",
      "iter step 877 training loss 8.484841\n",
      "iter step 878 training loss 8.484840\n",
      "iter step 879 training loss 8.484841\n",
      "iter step 880 training loss 8.484840\n",
      "iter step 881 training loss 8.484841\n",
      "iter step 882 training loss 8.484840\n",
      "iter step 883 training loss 8.484840\n",
      "iter step 884 training loss 8.484841\n",
      "iter step 885 training loss 8.484841\n",
      "iter step 886 training loss 8.484840\n",
      "iter step 887 training loss 8.484841\n",
      "iter step 888 training loss 8.484841\n",
      "iter step 889 training loss 8.484840\n",
      "iter step 890 training loss 8.484841\n",
      "iter step 891 training loss 8.484841\n",
      "iter step 892 training loss 8.484841\n",
      "iter step 893 training loss 8.484841\n",
      "iter step 894 training loss 8.484841\n",
      "iter step 895 training loss 8.484841\n",
      "iter step 896 training loss 8.484841\n",
      "iter step 897 training loss 8.484841\n",
      "iter step 898 training loss 8.484841\n",
      "iter step 899 training loss 8.484842\n",
      "iter step 900 training loss 8.484841\n",
      "iter step 901 training loss 8.484841\n",
      "iter step 902 training loss 8.484841\n",
      "iter step 903 training loss 8.484842\n",
      "iter step 904 training loss 8.484841\n",
      "iter step 905 training loss 8.484842\n",
      "iter step 906 training loss 8.484841\n",
      "iter step 907 training loss 8.484840\n",
      "iter step 908 training loss 8.484840\n",
      "iter step 909 training loss 8.484841\n",
      "iter step 910 training loss 8.484840\n",
      "iter step 911 training loss 8.484841\n",
      "iter step 912 training loss 8.484840\n",
      "iter step 913 training loss 8.484841\n",
      "iter step 914 training loss 8.484841\n",
      "iter step 915 training loss 8.484841\n",
      "iter step 916 training loss 8.484841\n",
      "iter step 917 training loss 8.484841\n",
      "iter step 918 training loss 8.484841\n",
      "iter step 919 training loss 8.484840\n",
      "iter step 920 training loss 8.484840\n",
      "iter step 921 training loss 8.484841\n",
      "iter step 922 training loss 8.484840\n",
      "iter step 923 training loss 8.484840\n",
      "iter step 924 training loss 8.484841\n",
      "iter step 925 training loss 8.484840\n",
      "iter step 926 training loss 8.484840\n",
      "iter step 927 training loss 8.484841\n",
      "iter step 928 training loss 8.484840\n",
      "iter step 929 training loss 8.484840\n",
      "iter step 930 training loss 8.484841\n",
      "iter step 931 training loss 8.484840\n",
      "iter step 932 training loss 8.484840\n",
      "iter step 933 training loss 8.484841\n",
      "iter step 934 training loss 8.484840\n",
      "iter step 935 training loss 8.484840\n",
      "iter step 936 training loss 8.484840\n",
      "iter step 937 training loss 8.484840\n",
      "iter step 938 training loss 8.484840\n",
      "iter step 939 training loss 8.484840\n",
      "iter step 940 training loss 8.484840\n",
      "iter step 941 training loss 8.484840\n",
      "iter step 942 training loss 8.484840\n",
      "iter step 943 training loss 8.484841\n",
      "iter step 944 training loss 8.484841\n",
      "iter step 945 training loss 8.484841\n",
      "iter step 946 training loss 8.484841\n",
      "iter step 947 training loss 8.484841\n",
      "iter step 948 training loss 8.484841\n",
      "iter step 949 training loss 8.484841\n",
      "iter step 950 training loss 8.484841\n",
      "iter step 951 training loss 8.484841\n",
      "iter step 952 training loss 8.484841\n",
      "iter step 953 training loss 8.484841\n",
      "iter step 954 training loss 8.484841\n",
      "iter step 955 training loss 8.484841\n",
      "iter step 956 training loss 8.484841\n",
      "iter step 957 training loss 8.484841\n",
      "iter step 958 training loss 8.484841\n",
      "iter step 959 training loss 8.484841\n",
      "iter step 960 training loss 8.484841\n",
      "iter step 961 training loss 8.484841\n",
      "iter step 962 training loss 8.484841\n",
      "iter step 963 training loss 8.484841\n",
      "iter step 964 training loss 8.484841\n",
      "iter step 965 training loss 8.484841\n",
      "iter step 966 training loss 8.484841\n",
      "iter step 967 training loss 8.484841\n",
      "iter step 968 training loss 8.484841\n",
      "iter step 969 training loss 8.484841\n",
      "iter step 970 training loss 8.484841\n",
      "iter step 971 training loss 8.484841\n",
      "iter step 972 training loss 8.484841\n",
      "iter step 973 training loss 8.484841\n",
      "iter step 974 training loss 8.484841\n",
      "iter step 975 training loss 8.484841\n",
      "iter step 976 training loss 8.484841\n",
      "iter step 977 training loss 8.484841\n",
      "iter step 978 training loss 8.484841\n",
      "iter step 979 training loss 8.484841\n",
      "iter step 980 training loss 8.484841\n",
      "iter step 981 training loss 8.484841\n",
      "iter step 982 training loss 8.484841\n",
      "iter step 983 training loss 8.484841\n",
      "iter step 984 training loss 8.484841\n",
      "iter step 985 training loss 8.484841\n",
      "iter step 986 training loss 8.484841\n",
      "iter step 987 training loss 8.484841\n",
      "iter step 988 training loss 8.484841\n",
      "iter step 989 training loss 8.484841\n",
      "iter step 990 training loss 8.484841\n",
      "iter step 991 training loss 8.484841\n",
      "iter step 992 training loss 8.484841\n",
      "iter step 993 training loss 8.484841\n",
      "iter step 994 training loss 8.484841\n",
      "iter step 995 training loss 8.484841\n",
      "iter step 996 training loss 8.484841\n",
      "iter step 997 training loss 8.484841\n",
      "iter step 998 training loss 8.484841\n",
      "iter step 999 training loss 8.484841\n",
      "0.038535148\n",
      "0.5069107\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"train a simple linear model by yourself !\")\n",
    "\n",
    "# model parameters\n",
    "# 对于简单的线性回归而言，参数初始值对结果基本没影响\n",
    "W = tf.Variable(0.3,  tf.float32)\n",
    "b = tf.Variable(-0.3, tf.float32)\n",
    "\n",
    "# model inputs & outputs\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# the model\n",
    "out = W * x + b\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_sum(tf.square(out - y))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = np.random.random_sample((100,)).astype(np.float32)\n",
    "y_train = np.random.random_sample((100,)).astype(np.float32)\n",
    "\n",
    "# training\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1000):\n",
    "        sess.run(train, {x:x_train, y:y_train})\n",
    "        current_loss = sess.run(loss, {x:x_train, y:y_train})\n",
    "        print(\"iter step %d training loss %f\" % (i, current_loss))\n",
    "    print(sess.run(W))\n",
    "    print(sess.run(b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
