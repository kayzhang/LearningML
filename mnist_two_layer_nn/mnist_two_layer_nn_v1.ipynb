{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# each image is 28*28\n",
    "# load the dataset\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot = True) # labels are \"one-hot vectors\"\n",
    "batch_size = 128\n",
    "n_hidden_1 = 256\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "\n",
    "# define input and output\n",
    "X = tf.placeholder(tf.float32, [None, n_input], name = \"X_placeholder\")\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes], name = \"Y_placeholder\")\n",
    "\n",
    "# define weights and bias\n",
    "weights = {\n",
    "    \"hidden\": tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev = 0.1)),\n",
    "    \"output\": tf.Variable(tf.zeros([n_hidden_1, n_classes]))\n",
    "}\n",
    "bias = {\n",
    "    \"hidden\": tf.Variable(tf.zeros(n_hidden_1)),\n",
    "    \"output\": tf.Variable(tf.zeros(n_classes))\n",
    "}\n",
    "\n",
    "def multilayer_nn(_X, _weights, _bias):\n",
    "    # hidden layer\n",
    "    hidden_layer = tf.nn.relu(tf.add(tf.matmul(_X, _weights[\"hidden\"]), _bias[\"hidden\"]))\n",
    "    # output layer\n",
    "    output_layer = tf.add(tf.matmul(hidden_layer, _weights[\"output\"]), _bias[\"output\"])\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "logits = multilayer_nn(X, weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax will convert logits and labels to probability\n",
    "# cross_entropy will compute the difference between predicted probability and real probability\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y, name = \"loss\")\n",
    "# get average of entropy\n",
    "loss = tf.reduce_mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer, use gradient descent\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0:1.26423291615395\n",
      "Average loss epoch 1:0.6167670371649149\n",
      "Average loss epoch 2:0.4843746109025462\n",
      "Average loss epoch 3:0.4253932987893378\n",
      "Average loss epoch 4:0.3887397748676491\n",
      "Average loss epoch 5:0.36445120379919216\n",
      "Average loss epoch 6:0.34748346367221333\n",
      "Average loss epoch 7:0.3324965895750584\n",
      "Average loss epoch 8:0.31976560234050927\n",
      "Average loss epoch 9:0.3100554521058823\n",
      "Average loss epoch 10:0.30120934771768976\n",
      "Average loss epoch 11:0.2928723102752423\n",
      "Average loss epoch 12:0.2858120333273094\n",
      "Average loss epoch 13:0.27863784759611515\n",
      "Average loss epoch 14:0.2737267416118186\n",
      "Average loss epoch 15:0.2664594381769776\n",
      "Average loss epoch 16:0.26109934322067074\n",
      "Average loss epoch 17:0.256348102103858\n",
      "Average loss epoch 18:0.2504910163251392\n",
      "Average loss epoch 19:0.2471420410108733\n",
      "Average loss epoch 20:0.24244184630525695\n",
      "Average loss epoch 21:0.2376908159582487\n",
      "Average loss epoch 22:0.23264171778808385\n",
      "Average loss epoch 23:0.23040798367250773\n",
      "Average loss epoch 24:0.22569641298972642\n",
      "Average loss epoch 25:0.2208544887376554\n",
      "Average loss epoch 26:0.21869927622152097\n",
      "Average loss epoch 27:0.21449207559679495\n",
      "Average loss epoch 28:0.21043794367696855\n",
      "Average loss epoch 29:0.2074058220346213\n",
      "Average loss epoch 30:0.2043564003351685\n",
      "Average loss epoch 31:0.20123371205755047\n",
      "Average loss epoch 32:0.19758427763110273\n",
      "Average loss epoch 33:0.19381392507494746\n",
      "Average loss epoch 34:0.19400134342255848\n",
      "Average loss epoch 35:0.18888980473254943\n",
      "Average loss epoch 36:0.1862436609737801\n",
      "Average loss epoch 37:0.18388523741479798\n",
      "Average loss epoch 38:0.18143541543008565\n",
      "Average loss epoch 39:0.17815167831131073\n",
      "Average loss epoch 40:0.17635183593818357\n",
      "Average loss epoch 41:0.17310297834775942\n",
      "Average loss epoch 42:0.17155466228723526\n",
      "Average loss epoch 43:0.16824348732863828\n",
      "Average loss epoch 44:0.16725591773336584\n",
      "Average loss epoch 45:0.16406061984859147\n",
      "Average loss epoch 46:0.1625992247187869\n",
      "Average loss epoch 47:0.1606273167095818\n",
      "Average loss epoch 48:0.15846479018077705\n",
      "Average loss epoch 49:0.15565970255222633\n",
      "Average loss epoch 50:0.15471650944862053\n",
      "Average loss epoch 51:0.15036891705610536\n",
      "Average loss epoch 52:0.15171723161543998\n",
      "Average loss epoch 53:0.14841707001303459\n",
      "Average loss epoch 54:0.14704153835252448\n",
      "Average loss epoch 55:0.14550818844170837\n",
      "Average loss epoch 56:0.1427559481981473\n",
      "Average loss epoch 57:0.1429206034242412\n",
      "Average loss epoch 58:0.14034972877387122\n",
      "Average loss epoch 59:0.13802421598376094\n",
      "Average loss epoch 60:0.1373820717344473\n",
      "Average loss epoch 61:0.1362977420220842\n",
      "Average loss epoch 62:0.13341262418177577\n",
      "Average loss epoch 63:0.1316453246074123\n",
      "Average loss epoch 64:0.1337762454607131\n",
      "Average loss epoch 65:0.1286716400830201\n",
      "Average loss epoch 66:0.12949106005174574\n",
      "Average loss epoch 67:0.12544912856185075\n",
      "Average loss epoch 68:0.12727589949548662\n",
      "Average loss epoch 69:0.12323896478349235\n",
      "Average loss epoch 70:0.12349163251114892\n",
      "Average loss epoch 71:0.1226447157101545\n",
      "Average loss epoch 72:0.12087269756065938\n",
      "Average loss epoch 73:0.11993092757873323\n",
      "Average loss epoch 74:0.11900598175458975\n",
      "Average loss epoch 75:0.11685042636412563\n",
      "Average loss epoch 76:0.11544513438843486\n",
      "Average loss epoch 77:0.11628454694381127\n",
      "Average loss epoch 78:0.11413967368433943\n",
      "Average loss epoch 79:0.11293399597937133\n",
      "Average loss epoch 80:0.1123332660428155\n",
      "Average loss epoch 81:0.10979168715350555\n",
      "Average loss epoch 82:0.11000952763713046\n",
      "Average loss epoch 83:0.1090817472665182\n",
      "Average loss epoch 84:0.10749574406788899\n",
      "Average loss epoch 85:0.10863189144811153\n",
      "Average loss epoch 86:0.10409430214714059\n",
      "Average loss epoch 87:0.1058681402008086\n",
      "Average loss epoch 88:0.1035538070199095\n",
      "Average loss epoch 89:0.10282565897334983\n",
      "Average loss epoch 90:0.1031892453179215\n",
      "Average loss epoch 91:0.10201196391178873\n",
      "Average loss epoch 92:0.0992017557067332\n",
      "Average loss epoch 93:0.10114554775605251\n",
      "Average loss epoch 94:0.09844324199834487\n",
      "Average loss epoch 95:0.09832074559599786\n",
      "Average loss epoch 96:0.09709267176705084\n",
      "Average loss epoch 97:0.09571870951316296\n",
      "Average loss epoch 98:0.0951898918858358\n",
      "Average loss epoch 99:0.09478120642143252\n",
      "Total time: 109.1196882724762 seconds\n"
     ]
    }
   ],
   "source": [
    "# start session\n",
    "n_epochs = 100\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    n_batches = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict = {X: X_batch, Y: Y_batch})\n",
    "            total_loss += loss_batch\n",
    "        print(\"Average loss epoch {0}:{1}\".format(i, total_loss / n_batches))\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
