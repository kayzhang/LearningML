{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-CIFAR-10\n",
    "Coach: [Harry](https://saoyan.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "CIFAR-10 classification is a common benchmark problem in machine learning. The problem is to classify RGB 32x32 pixel images across 10 categories:\n",
    "\n",
    "airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "Refer to [CIFAR-10 webpage](https://www.cs.toronto.edu/~kriz/cifar.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "\n",
    "The CIFAR-10 data-set is about 163 MB and will be downloaded automatically if it is not located in the given path. It might take a few minutes to download. Just wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10\n",
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the class-names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = cifar10.load_class_names()\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training-set. This returns the images, the class-numbers as integers, and the class-numbers as One-Hot arrays called labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, cls_train, labels_train = cifar10.load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images_train.shape)\n",
    "print(cls_train.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test, cls_test, labels_test = cifar10.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images_test.shape)\n",
    "print(cls_test.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 data-set has now been loaded and consists of 60,000 images and associated labels (i.e. classifications of the images). The data-set is split into 2 mutually exclusive sub-sets, the training-set and the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(images_train)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(images_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10 import img_size, num_channels, num_classes\n",
    "print(img_size)\n",
    "print(num_channels) # RGB\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a few images to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(10):\n",
    "    img = images_train[i]\n",
    "    label = labels_train[i]\n",
    "    \n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    cls = np.argmax(label)\n",
    "    cls_name = class_names[cls]\n",
    "    print(cls_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a two-layer neural network on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weight(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, padding='SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = padding)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the compuation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the model (build the compuation graph)\n",
    "## Hyperparameters\n",
    "batch_size = 64\n",
    "W_input = 32 # width of the image\n",
    "H_input = 32 # height of the image\n",
    "C_input = 3  # channel of the image\n",
    "W1 = 5; H1 = 5; C1 = 32 # conv-1\n",
    "W2 = 5; H2 = 5; C2 = 64 # conv-2\n",
    "C3 = 512\n",
    "n_classes = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "## Model input\n",
    "x = tf.placeholder(tf.float32, [None, W_input, H_input, C_input])\n",
    "\n",
    "# 1st conv layer\n",
    "W_conv1 = weight([W1,H1,C_input,C1])\n",
    "b_conv1 = bias([C1])\n",
    "h_conv1 = conv2d(x, W_conv1) + b_conv1\n",
    "h_conv1 = tf.nn.relu(h_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1) # 16*16*C1\n",
    "\n",
    "# 2nd conv layer\n",
    "W_conv2 = weight([W2,H2,C1,C2])\n",
    "b_conv2 = bias([C2])\n",
    "h_conv2 = conv2d(h_pool1, W_conv2) + b_conv2\n",
    "h_conv2 = tf.nn.relu(h_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) # feature size: 8x8xC2\n",
    "\n",
    "# densely connected layer with 1024 neurons\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*C2])\n",
    "W_fc = weight([8*8*C2,C3])\n",
    "b_fc = bias([C3])\n",
    "h_fc = tf.matmul(h_pool2_flat, W_fc) + b_fc\n",
    "h_fc = tf.nn.relu(h_fc)\n",
    "\n",
    "# dropout layer\n",
    "p_keep = tf.placeholder(tf.float32)\n",
    "h_fc = tf.nn.dropout(h_fc, p_keep)\n",
    "\n",
    "# output layer\n",
    "W_output = weight([C3,10])\n",
    "b_output = bias([10])\n",
    "y_pred = tf.matmul(h_fc, W_output) + b_output\n",
    "\n",
    "\n",
    "## Define loss and optimizer\n",
    "y_gt = tf.placeholder(tf.float32, [None, n_classes])\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=y_gt))\n",
    "\n",
    "## Train (update model parameters)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "## Compute Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_gt, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to get a random training-batch\n",
    "\n",
    "There are 50,000 images in the training-set. It takes a long time to calculate the gradient of the model using all these images. We therefore only use a small batch of images in each iteration of the optimizer.\n",
    "\n",
    "If your computer crashes or becomes very slow because you run out of RAM, then you may lower the batch size, but you may then need to perform more optimization iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(images_train, labels_train, batch_size):\n",
    "    # Number of images in the training-set.\n",
    "    num_images = len(images_train)\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = images_train[idx]\n",
    "    y_batch = labels_train[idx]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "max_iter = 10000\n",
    "\n",
    "images_train = images_train.reshape([50000, 32, 32, 3])\n",
    "images_test = images_test.reshape([10000, 32, 32, 3])\n",
    "\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    batch_x, batch_y = random_batch(images_train, labels_train, batch_size)\n",
    "    sess.run(train_step, feed_dict = {x: batch_x, y_gt: batch_y, p_keep:0.5})\n",
    "    if iter % 100 == 0:\n",
    "        train_loss = sess.run(loss, feed_dict = {x: batch_x, y_gt: batch_y, p_keep:1.0})\n",
    "        train_accuracy = sess.run(accuracy, feed_dict = {x: batch_x, y_gt: batch_y, p_keep:1.0})\n",
    "\n",
    "        test_accuracy = 0\n",
    "        for k in range(5):\n",
    "            test_x = images_test[k*2000:(k+1)*2000,:]\n",
    "            test_y = labels_test[k*2000:(k+1)*2000,:]\n",
    "            test_accuracy += sess.run(accuracy, {x: test_x, y_gt: test_y, p_keep:1.0})\n",
    "        test_accuracy /= 5\n",
    "\n",
    "        print(\"iter step %d, loss %f, training accuracy %f, test accuracy %f\" %\n",
    "              (iter, train_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use some examples to test your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = images_train.reshape([50000, 32, 32, 3])\n",
    "images_test = images_test.reshape([10000, 32, 32, 3])\n",
    "for i in range(10):\n",
    "    img = images_test[i]\n",
    "    label = labels_test[i]\n",
    "    \n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    cls = np.argmax(label)\n",
    "    cls_name = class_names[cls]\n",
    "    print(\"Ground Truth: %s\" % cls_name)\n",
    "    \n",
    "    pred_label = sess.run(y_pred, feed_dict = {x: img.reshape([1, 32, 32, 3]), p_keep:1.0})\n",
    "    pred_cls = np.argmax(pred_label)\n",
    "    pred_cls_name = class_names[pred_cls]\n",
    "    print(\"Model prediction: %s\" % pred_cls_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
